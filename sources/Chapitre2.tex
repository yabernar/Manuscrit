%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%									Chapitre 2												%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Détection de Nouveauté}
	\citationChap{
		D'abord, j'observe les êtres humains car je les aime bien. J'enregistre dans ma tête tout ce que j'ai remarqué, et ensuite, avec les souvenirs de ce que j'ai vu, je dessine.
	}{Hayao Miyazaki}
	\minitoc
	\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% Début du chapitre
			
	\section{Introduction}

	Ce chapitre présente notre méthode pour réaliser une détection de nouveauté avec les cartes auto-organisatrices (SOM), ou plus généralement, avec des modèles de quantification vectorielle avec topologie.

	Nous commencerons par montrer différentes façons d'apprendre les images avec des SOM et celle que nous avons choisie. Puis nous introduirons notre méthode de détection de nouveauté en deux parties : d'abord avec quantification vectorielle, puis une seconde avec la topologie. Ensuite nous aborderons le protocole expérimental que nous avons mis en œuvre pour évaluer notre modèle. La dernière section présente nos résultats.

	\section{Application aux images}\label{sec:images}

	Il existe de différentes possibilités pour apprendre des images avec la quantification vectorielle, dû aux multiples façons de découper une image en vecteurs d'apprentissage. Pour présenter l'impact que peut avoir ce découpage, on peut considérer deux extrêmes : apprendre l'image en entier ou apprendre chaque pixel individuellement. Le premier cas peut sembler absurde dans le cas d'une seule image (une base de données d'un seul élément), mais peut présenter un intérêt lorsque l'on considère une suite d'images par exemple. Dans ce cas, l'environnement appris serait défini par l'entièreté de ce que voit le capteur et tous les changements, où qu'ils soient dans l'image, auraient de l'importance.

	Le second extrême est l'apprentissage au niveau du pixel. Dans cette approche chaque pixel est un vecteur d'entrée, ce qui rendrait l'espace d'entrée unidimensionnel pour une image en niveau de gris (ou tridimensionnel si l'image est en couleur, plus de détails dans la section \ref{sec:img:colors}). Sur un plan conceptuel, ce choix considère qu'une image est définie uniquement par les couleurs ou luminosités présentes, peu importe leur positions dans celle-ci. C'est utilisé notamment dans SOBS (Self-organized background substraction) \cite{maddalena2008self}, et toutes ses déclinaisons ultérieures. Il existe beaucoup d'autres découpages possibles entre ces deux extrêmes, chacun représentant une certaine façon de conceptualiser une image et définissant la notion de nouveauté. 
	Le premier cas intègre la luminance, la couleur, et leur répartition spatiale dans le vecteur d'apprentissage alors que le deuxième ignore la relation spatiale entre les luminances/couleurs. Ainsi le premier modèle est spécifique à la situation apprise et sera très sensible aux mouvements des objets dans la scène. Le deuxième modèle est plus générique en réagissant aux objets dont la nature colorimétrique ou de luminance aura changé.

	Il est important donc de considérer le contexte de nos travaux pour définir la façon de représenter une image, notre application étant la detection de nouveauté. Dans ce contexte, nous considérons qu'une image est une combinaison de nombreux éléments plus petits. Par exemple, une photographie d'un lac de montagne \ref{fig:img:nino} peut être décrite comme étant la combinaison d'un élément de lac (avec sa couleur, bleu sombre et sa texture uniforme), d'un élément de plaine herbeuse (verte et uniforme), d'éléments rocailleux qui sont gris et soit uniformes (dans le premier plan) soit plus contrastés en se combinant avec la verdure de la végétation (dans les bords de l'image), et ainsi de suite. Ce découpage "sémantique" de l'image est ce qui permet la détection de nouveauté  de fonctionner, car dans notre cas la nouveauté est par définition ce qui n'est pas déjà dans l'image et donc ne faisant pas partie de ces classes d'éléments. Pour regrouper les parties d'une image appartenant au même élément, il est nécessaire d'avoir une information mise en contexte (un pixel seul ne suffit généralement pas à savoir à quel élément il appartient dans l'image), ce qui implique que les pixels doivent être pris dans leurs environnements locaux pour conserver l'information de voisinage comme la texture par exemple. Nous avons donc choisi de découper l'image en plus petites images à la façon d'une mosaïque, que nous appellerons des imagettes. Ces imagettes (d'une taille arbitraire en hauteur et largeur) conservent l'environnement local tout en étant suffisamment petites pour être précises dans l'espace. Une imagette ne représentera qu'une partie d'un élément et non pas regroupant plusieurs éléments, ce qui réduirait sa capacité de généralisation. Mais elles permettent aussi d'avoir une base d'apprentissage suffisamment étoffée pour tirer parti des propriétés de nos modèles de quantification vectorielle et de leur topologie. La partie pratique de l'apprentissage d'une image, sa représentation et sa reconstruction sont abordés dans la section suivante \ref{sec:img:compression}.


	\begin{figureth}
		\includegraphics[width=\linewidth]{nino_mod}
		\caption[Lac de Nino]{Exemple d'image comportant plusieurs éléments notables tels qu'un lac (bleu sombre et uniforme), une plaine herbeuse (verte et uniforme), d'éléments rocailleux qui sont gris et soit uniformes (dans le premier plan) soit plus contrastés en se combinant avec la verdure de la végétation (dans les bords de l'image), et ainsi de suite. Une image peut donc être vue comme un ensemble de zones différentes composées d'éléments proches entre eux.\footnotemark}\label{fig:img:nino}
	\end{figureth}

	\footnotetext{Image source : Lac de Nino, https://fr.wikipedia.org/wiki/Lac\_de\_Nino\#/media/Fichier:Lac\_de\_Nino.jpg, CC BY-SA}

	\subsection{Apprentissage et reconstruction}\label{sec:img:compression}
	\subsubsection{Apprentissage}
	La première étape nécessaire à l'apprentissage est la constitution de la base d'apprentissage à partir de l'image. L'utilisation de modèles de quantification vectorielle établissent la première contrainte pour le découpage : les imagettes doivent être d'une taille fixe. En effet il est nécessaire pour les SOM comme pour les GNG et leurs variantes que tous les vecteurs d'entrées soient de la même longueur pour que le calcul de distance avec les neurones fonctionne, pour qu'ils représentent le plus fidèlement possible les entrées qui leurs sont attachées.

	Nous avons également choisi de limiter nos tailles d'imagettes à des carrés, pour des raisons de simplicité. Il est possible que des imagettes plus larges que hautes ou plus hautes que larges représentent mieux les éléments de l'image que l'on apprend. Cependant ce serait une préférence spécifique à chaque image et peu généralisable, car si on effectue par exemple une rotation de 90° de l'image, la préférence s'inversera. L'inversement des tailles dans les imagettes carrées ne changeant rien, elles sont pour leur part insensibles aux rotations discrètes de l'image (par pas de 90°).

	Dans la version classique \cite{amerijckx-compression}, le découpage de l'image se fait en mosaïque, sans superpositions entre les imagettes. C'est à dire que chaque pixel n'appartient qu'à une seule imagette. Le processus est montré sur la figure \ref{fig:img:dec} Si les dimensions de l'images ne sont pas un multiple de la taille des imagettes, les pixels en trop sont rognés par la droite et par le bas.

	\begin{figureth}
		\includegraphics[width=\linewidth]{image_representation}
		\caption[Représentation d'une image]{Illustration du processus de représentation et d'apprentissage d'une image par une SOM.}\label{fig:img:dec}
	\end{figureth}

	\subsubsection{Reconstruction}

	Une fois l'apprentissage terminé, on obtient deux sorties. La première est le modèle entraîné avec les différents poids des neurones codant une imagette représentante du cluster d'imagettes associé à ce neurone. La seconde est la liste, pour chaque imagette de l'image, de l'indice du neurone le plus proche de celle-ci, qui pourra être utilisé pour la reconstruction de l'image d'apprentissage. Le procédé est montré sur la figure \ref{fig:img:comp_ex}.

	Pour reconstruire une image à partir de la liste des indices de BMU, il suffit de remplacer chaque indice par le vecteur prototype du neurone auquel il correspond. Ces vecteurs prototypes devront être réassemblés en imagettes (dans un tableau à 2 dimensions à la place d'un vecteur à une dimension), et seront placés à la bonne position pour reformer l'image.

	Il est aussi possible de reconstruire une image qui n'a pas été apprise. Pour cela il faut créer la liste d'indices des BMU des imagettes de la nouvelle image, et de reconstruire ensuite l'image par le même procédé que montré précédemment. Il faut noter que l'image que l'on souhaite reconstituer doit être proche de l'image apprise, c'est à dire contenir des éléments similaires, pour obtenir un résultat correct.

	\begin{figureth}
		\begin{subfigureth}{\textwidth}
			\includegraphics[width=\linewidth]{triangle/Image_compression2}
		\end{subfigureth}
		\begin{subfigureth}{\textwidth}
			\includegraphics[width=0.2\linewidth]{triangle/triangle_original}\hfill
			\includegraphics[width=0.2\linewidth]{triangle/triangle_grid}\hfill
			\includegraphics[width=0.2\linewidth]{triangle/som}\hfill
			\includegraphics[width=0.2\linewidth]{triangle/triangle}
		\end{subfigureth}
		\caption[Compression et décompression d'image]{Schéma simplifié du processus de compression et de reconstruction d'une image, avec ici seulement 9 neurones et 25 imagettes.}\label{fig:img:comp_ex}
	\end{figureth}

	\subsection{La gestion des canaux (couleurs)}\label{sec:img:colors}

	Nous avons jusque là vu comment apprendre une image ayant une seule valeur par pixel (soit des images en nuances de gris). Cependant la majorité des dispositifs de capture actuels fournissent des images en couleurs, c'est à dire avec trois canaux. Nous allons voir dans cette section comment transposer l'apprentissage, la compression et la reconstruction à des images à un nombre arbitraire de canaux par pixels.

	Une approche possible serait de séparer l'image par composante. Une image RVB par exemple donnerait trois images en niveau de gris, une R, une V et une B. Deux options s'offrent ensuite à nous. Soit utiliser une seule SOM pour apprendre toutes les images ainsi extraites en espérant que les différentes formes présentes dans chaque composante soient assez similaires entre elles. Cela augmente aussi les données que la SOM doit apprendre, car on vient de multiplier la taille de notre base d'apprentissage par le nombre de canaux. Ces données doivent être aussi cohérentes dans l'espace d'entrée pour que la réduction dimensionnelle se fasse correctement. Soit utiliser une SOM par composante pour apprendre chaque canal séparément, et de regrouper ensuite les différents canaux reconstitués en une image couleur. 
	
	Cependant ces deux approches ont un défaut majeur pour la compression d'images (ainsi que la détection de nouveauté en conséquence), c'est la création d'aberrations chromatiques dans l'image reconstituée. En effet, les canaux étant appris séparément avant d'être recombinés, la reconstruction peut donner pour certains pixels des teintes de couleurs qui n'existaient pas dans l'image de base, qui peuvent être très saillants visuellement. Par exemple un pixel blanc dans l'image d'entrée (avec une forte composante R, V et B), lorsque reconstitué par la SOM peut être bien reconstitué dans deux composantes (V et B par exemple), et mal reconstitué dans la troisième (R) avec une valeur beaucoup plus faible que dans l'image de base, car on minimise la distance entre les canaux séparément. Par conséquent ce pixel aura une couleur turquoise dans l'image reconstituée à la place de blanc. Cette erreur minimise bien la distance avec l'image de base, et ce n'est que visuellement que les changements de teintes sont apparents et dégradent proportionnellement plus la qualité de l'image que l'erreur mesurée.

	Une meilleure approche consiste à inclure tous les canaux directement dans les imagettes. Chaque imagette devient donc une imagette en couleur, et sa taille augmente donc en conséquence. Une imagette de 10 par 10 pixels par exemple qui donnerait un vecteur prototype de taille 100, passe à 300 avec les trois couleurs. L'apprentissage et la reconstruction se déroulent de la même manière que dans la SOM classique. Il faut aussi noter que l'ordre n'a pas d'importance dans les vecteurs prototypes, on peut arranger les valeurs en RGBRGBRGB tout comme RRRGGGBBB sans que cela ne change le résultat, le calcul de distance euclidienne étant indépendant de l'ordre des coordonnées. La figure \ref{fig:img:rep} montre une comparaison de reconstructions obtenues entre couleurs fusionnées et couleurs séparées.

	\begin{figureth}
		\begin{subfigureth}{0.48\textwidth}
			\includegraphics[width=\linewidth]{flower_low_20}\caption{Couleurs fusionnées}	
		\end{subfigureth}
		\begin{subfigureth}{0.48\textwidth}
			\includegraphics[width=\linewidth]{flower_added_20}\caption{Couleurs séparées apprises par 3 SOM}	
		\end{subfigureth}
		\caption[Représentation d'une image]{Comparaison entre une image avec des couleurs fusionnées et la même image avec des couleurs séparées qui présente des artefacts visuels.\footnotemark}\label{fig:img:rep}
	\end{figureth}

	\newpage
	\section{Détection de nouveauté par QVAT : quantification vectorielle et apprentissage topologique}

	\footnotetext{Image source: https://unsplash.com/photos/9A\_peGrSbZc}

	Nous avons à partir des différentes propriétés de nos modèles neuronaux et de l'apprentissage d'images, développé des processus de détection de nouveauté. Ces processus ne sont pas intrinsèques à nos modèles, c'est à dire que nos modèles neuronaux n'ont pas été définis dans le but d'effectuer une détection de nouveauté. Elle est une propriété émergente de ces modèles. Nous présentons ces deux méthodes dans cette section. La première est basée sur la propriété de quantification vectorielle et la seconde sur la topologie des SOM, avec une modulation par ce que nous appelons la distance neurale. La première a été introduite dans \cite{bernard2019novelty}, et complémentée par la seconde dans \cite{bernard2020novelty}.

	\subsection{Détection avec quantification vectorielle}

	\begin{figureth}
		\includegraphics[width=\linewidth]{saliency_map_generation}
		\caption[Détection de nouveauté par quantification vectorielle]{On peut observer qu'il y a eu deux changements entre le fond et l'image perçue : un avion est apparu et les nuages ont bougé. Les nuages, déjà présents dans le fond sont bien reconstruits. L'avion cependant est nouveau, et n'est pas bien reconstruit. Ainsi la différence entre l'image perçue et la reconstruction rend plus saillant l'avion que les nuages. Contrairement à une simple différence entre le fond et l'image perçue, où les deux seraient saillants. Nous avons représenté le modèle appris comme étant une SOM sur cette figure, cependant il peut s'agir de n'importe quel modèle de quantification vectorielle.}\label{fig:img:vq}
	\end{figureth}

	La détection par quantification vectorielle (VQ) repose sur l'erreur de reconstruction de la nouvelle image provenant du capteur. Cette erreur est la différence entre la nouvelle image et celle reconstruite avec le modèle de VQ ayant effectué son apprentissage sur l'image de fond.

	Il y a deux cas à considérer pour chaque imagette de la nouvelle image du capteur : soit la nouvelle imagette est similaire à une partie quelconque de l'arrière-plan appris, soit quelque chose de nouveau est présent dans l'imagette. Dans le premier cas, le neurone représentatif de cette imagette sera proche de celle-ci, c'est à dire que les poids du neurones et de l'imagette seront proches. L'erreur de reconstruction de cette imagette sera faible, surtout si la caméra est statique, mais avec une petite différence toujours présente en raison des pertes de la compression et des changements naturels de l'environnement visuel.
	
	Cependant, dans le second cas, le neurone représentatif de l'imagette sera éloigné de celle-ci, dans le sens où ses poids seront très différents de ceux de l'imagette. Car la nouveauté est par définition quelque chose qui n'était pas présent dans l'arrière-plan et donc quelque chose que le modèle n'a pas appris. Cela entraînera une différence significative entre l'image perçue et sa reconstruction à l'endroit de la nouveauté. Ce processus est illustré dans la figure \ref{fig:img:vq}, et des exemples de résultats dans la figure \ref{fig:visu-vq}. 

	Ce processus a l'avantage d'être précis au niveau du pixel pour la mise en évidence des changements, avec une certaine limite dûe à la taille des imagettes. Si les changements sont trop importants et que la nouvelle BMU est trop différente de celle de référence, alors il y a un risque de perte de précision au niveau de l'imagette. Il est aussi théoriquement insensible au déplacement d'objets du fond pour la détection de la nouveauté. Cependant, il peut être bruité en raison de l'apprentissage imparfait de la VQ et de la variabilité naturelle de l'environnement.

	\begin{figureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/diff/bkg}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/diff/in000580}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/diff/dif000580}
		\end{subfigureth}

		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/diff/back} \caption{Image de fond}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/diff/in002390} \caption{Image courante}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/diff/dif002390} \caption{Différence avec VQ}
		\end{subfigureth}

		\caption[Exemples de différence avec quantification vectorielle]{Exemples de différence avec quantification vectorielle. On peut remarquer que la couleur est un facteur important pour savoir si un objet nouveau sera correctement détecté.}\label{fig:visu-vq}
	\end{figureth}

	\subsection{Détection avec distance neurale}

	\begin{figureth}
		\includegraphics[width=\linewidth]{neural_distance_modulation_v2}
		\caption[Détection de nouveauté avec topologie]{Le processus présenté ici concerne une position dans l'image, et il est répété sur toute l'image pour obtenir la carte de distances neurales en bas. Nous avons représenté le modèle appris comme étant une SOM sur cette figure, cependant il peut s'agir de n'importe quel modèle avec une topologie regroupant les éléments proches.}\label{fig:img:topo}
	\end{figureth}

	La détection avec distance neurale se base sur les propriétés topologiques du modèle pour trouver la nouveauté dans une image. La topologie est ce qui relie les différents neurones de modèle par leur proximité : les neurones proches dans la topologie ont appris des poids similaires.

	Après avoir effectué l'apprentissage du modèle, nous mémorisons la liste des positions dans la carte des BMU trouvés pour toutes les imagettes. Lorsqu'une nouvelle image est présentée au capteur, nous effectuons le processus de reconstruction comme dans la première méthode de détection de la nouveauté. Mais nous nous intéressons uniquement à la liste des positions dans la carte des BMU trouvées pour toutes les imagettes de la nouvelle image. En comparant les deux listes, nous pouvons trouver des changements dans les positions des BMU pour chaque emplacement d'imagette. Si la BMU est la même entre le fond et la nouvelle image à un endroit, alors il n'y a probablement pas de nouveauté à cet endroit. S'il y a une différence entre les deux BMU, alors il pourrait y avoir de la nouveauté à cet endroit. Pour quantifier cette différence, nous calculons la distance topologique qui les sépare sur la carte. La distance topologique est définie par le nombre de nœuds qui sont traversés par le chemin le plus court entre les deux BMU dans la topologie de la carte. Dans la SOM classique basée sur une grille, il s'agit simplement de la distance L1.

	Grâce à ces distances, nous pouvons créer une carte de saillance où des distances élevées dans la topologie signifient des changements significatifs dans l'image, car la proximité dans la topologie signifie la proximité dans l'espace d'entrée. Cette méthode permet d'obtenir une carte de saillance plus robuste, avec moins de bruit que la version avec la VQ. La précision est limitée à la taille des imagettes et il n'y a pas d'inhibition pour les éléments déjà connus qui se sont déplacés dans le fond. Le processus est illustré dans la figure \ref{fig:img:topo}.

	\begin{figureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/neural/bkg}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/neural/in001438}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/neural/win001438}
		\end{subfigureth}

		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/neural/back} \caption{Image de fond}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/neural/in000884} \caption{Image courante}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{example_qvat/neural/win000884} \caption{Distance neurale}
		\end{subfigureth}

		\caption{Exemples de différence avec distance neurale.}\label{fig:visu-tp}
	\end{figureth}

	\subsection{Considérations pour la combinaison}\label{seq:combinaison}
	
	Une fois les deux cartes de saillance générées, il est nécessaire de les combiner pour n'avoir qu'un seul résultat qui représentera la sortie de notre système. Il existe un très grand nombre de façons de faire cette combinaison, et il existe dans la littérature des modèles qui se basent sur une bonne combinaison de différentes cartes de saillance pour obtenir de meilleurs résultats \cite{bianco2017combination}. Dans notre cas, nous avons préféré utiliser une combinaison simple de nos deux cartes de saillance. C'est à dire qu'elle n'utilise pas de paramètres, pour ne pas ajouter une variable de plus à optimiser. Nous souhaitons aussi bénéficier de la complémentarité des deux cartes de saillance. La solution la plus simple est de multiplier les deux cartes ensemble. Ainsi, sera considéré comme nouveauté dans la carte de sortie, ce qui apparaît comme nouveauté en même temps dans les deux cartes de saillance, sur le principe de $\textit{petit} \times \textit{petit} = \textit{petit}$, $\textit{grand} \times \textit{petit} = \textit{petit}$ et seulement $\textit{grand} \times \textit{grand} = \textit{grand}$, à l'instar d'un "et" logique. Le bruit présent dans la carte résultant de la quantification vectorielle et qui n'est pas présent dans la carte topologique disparaît de la carte finale. Il en va de même pour les mouvements qui ne sont pas des nouveautés qui apparaissent dans la carte topologique, mais pas dans la carte de quantification vectorielle.

	Un problème qui peut apparaître avec cette méthode est la trop petite valeur du résultat et le déséquilibre d'impact de nos deux cartes, car nos cartes de saillance sont toutes les deux définies entre 0 et 1. Il est possible qu'il existe des situations où les deux cartes ont un impact disproportionnel sur le résultat. Par exemple si la saillance a une valeur de 0.2 sur une carte et 0.8 sur l'autre, alors la seconde aura plus d'impact sur les valeurs de la sortie finale. De même, en multipliant deux nombres compris entre 0 et 1, le résultat sera forcément inférieur à chacun des deux nombres. Cela a un effet réducteur sur toutes les valeurs de la carte de saillance finale. La solution que nous avons choisi à ces deux problèmes, est de re-normaliser les deux cartes de saillances avant de les multiplier. Cela a pour effet d'éviter une trop grande disproportion d'impact entre les deux cartes.
	
	Cependant, cela vient aussi avec des désavantages, comme par exemple le fait que si il n'y a pas de signal dans l'entrée, le maximum des cartes de saillance sera quand même 1. On pourrait observer des signaux positifs dans la sortie alors que l'entrée et les cartes de saillances n'en montrent pas. En pratique, cela est peu fréquent car la valeur maximum dans un cas où il n'y a pas de signal en entrée vient du bruit de reconstruction, et est donc décorrélée entre les deux cartes car la distance neurale n'y est pas sujette, et disparaîtra lors de la multiplication.

	Une autre option aurait été de pondérer les deux cartes avant de les multiplier. Nous ne l'avons pas choisie car cela aurait eu pour effet de rendre la valeur de la nouveauté dans la carte de saillance très variable : différentes nouveautés pouvant générer des saillances différentes, par forcément représentatives de leur qualité de nouveauté. Or, nous ne nous intéressons pas à la valeur absolue de la nouveauté, qui change en fonction de la séquence évaluée, mais à la différence relative entre la nouveauté et le fond. Ce qui ne peut pas être obtenu avec une pondération.

	\newpage
	\section{Protocole expérimental}

	Cette section regroupe l'ensemble des considérations pratiques pour la réalisation de nos expériences. Nous présenterons la base de donnée utilisée, comment ces données ont été préparées, les différentes métriques que nous avons utilisées et les paramétrages de nos modèles.

	\subsection{Présentation de la base de données}

	Nous avons choisi d'utiliser la base de données ChangeDetection.Net (CDnet) \cite{wang-cdnet} pour réaliser nos expériences. C'est un jeu de données très utilisé dans le domaine de la détection de changements dans le domaine visible et infrarouge avec une caméra statique. Ce sont des données tirées de situations réelles par le biais de caméras de surveillance par exemple, et dédiées à la détection de changements.

	Nous avons dans cette thèse avant tout parlé de détection de nouveauté, et il existe une différence conceptuelle entre la nouveauté et le changement. La détection de changement se concentre sur le mouvement pour séparer le fond des objets intéressants dans une image. La détection de nouveauté quant à elle se réfère à une représentation apprise de l'environnement. Dans les captures vidéos réelles, les deux sont généralement équivalentes dû au fait que lorsqu'une nouveauté apparaît, elle le fait généralement en se déplaçant. En pratique cela veut dire que la majorité, mais pas la totalité, de CDnet peut être utilisée pour de la détection de nouveauté. Nous présenterons les catégories et vidéos que l'on a utilisées, et des exemples de vidéos qui n'ont pas été retenues avec des explications dans la suite.
	
	CDnet regroupe 53 séquences vidéos d'origines variées. Elles proviennent principalement de caméras de surveillance ou de captures effectuées par des chercheurs pour leur propres besoins. Il y a des captures réelles, sans images synthétiques, de scènes intérieures et extérieures. Les vidéos sont toutes en couleur, sauf pour deux catégories \textit{thermal} et \textit{turbulences}, et de résolution assez faible, allant de $320 \times 240$ à $720 \times 486$ pixels. Elles sont groupées en 11 catégories de 4 à 6 vidéos représentant une variété de difficultés rencontrées sur le terrain. Il existe cependant un certain biais dans CDnet, car il est fortement orienté vers de la détection de personnes et de véhicules. Ces catégories sont présentées dans les figures suivantes.

	Parmi les 11 catégories de CDnet, nous avons décidé d'en utiliser 8 complètement, d'utiliser une version réduite pour 2 d'entre elles et d'en enlever une. Les deux catégories réduites sont \textit{Intermittent Object Motion} et \textit{Low framerate}. La réduction consiste à enlever une partie des vidéos qui ne correspondaient pas à notre application visée de détection de nouveauté et de conserver les autres. Une illustration de la différence entre la détection de changements et la détection de nouveauté, qui est la raison pour laquelle nous avons écartées ces vidéos, est montrée sur la figure \ref{fig:cdnet:diff}. La catégorie que nous avons décidé de ne pas du tout utiliser car elle ne correspondait pas à notre scénario est \textit{Pan tilt zoom}. C'est une catégorie un peu spéciale car elle change une partie fondamentale du scénario. La caméra n'est plus statique mais effectue des rotations et des zooms ce qui change significativement son environnement visuel.

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/baseline/in000277}\caption{Highway}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/baseline/in000895}\caption{Office}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/baseline/in000473}\caption{Pedestrians}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/baseline/in000118}\caption{PETS2006}	
		\end{subfigureth}
		\caption[Catégorie Baseline]{\textit{Baseline} : La catégorie de base qui comprend des scénarios typiques de détection de changement (traffic, piétons) sans difficultés particulières.}\label{fig:cdnet:baseline}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/badWeather/in000810}\caption{Snowfall}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/badWeather/in001951}\caption{Skating}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/badWeather/in002841}\caption{WetSnow}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/badWeather/in004863}\caption{Blizzard}	
		\end{subfigureth}
		\caption[Catégorie Bad Weather]{\textit{Bad Weather} : Cette catégorie comprend des variations du scénario de base avec une météo dégradée. La difficulté principale vient de la neige qui tombe, et du changement de l'environnement avec les traces de pneus sur la neige par exemple.}\label{fig:cdnet:badweather}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/cameraJitter/in000145}\caption{Sidewalk}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/cameraJitter/in000187}\caption{Badminton}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/cameraJitter/in000355}\caption{Traffic}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/cameraJitter/in002287}\caption{Boulevard}	
		\end{subfigureth}
		\caption[Catégorie Camera Jitter]{\textit{Camera Jitter} : Ces vidéos proviennent de caméras instables à cause de vent fort ou d'autres raisons. Elles ont de façon irrégulière des translations verticales et horizontales  rapides et de petite amplitude.}\label{fig:cdnet:jitter}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/dynamicBackground/in000716}\caption{Fountain01}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/dynamicBackground/in000720}\caption{Fountain02}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/dynamicBackground/in000937}\caption{Canoe}	
		\end{subfigureth}\\
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/dynamicBackground/in001504}\caption{Fall}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/dynamicBackground/in002519}\caption{Overpass}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/dynamicBackground/in007726}\caption{Boats}	
		\end{subfigureth}
		\caption[Catégorie Dynamic Background]{\textit{Dynamic Background} : La difficulté se porte sur le contenu du fond qui est changeant. Il peut s'agir d'eau ou d'arbres qui bougent dans le vent.}\label{fig:cdnet:dynamic}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/shadow/in000260}\caption{CopyMachine}
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/shadow/in000320}\caption{PeopleInShade}
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/shadow/in000358}\caption{Bungalows}	
		\end{subfigureth}\\
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/shadow/in001015}\caption{BusStation}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/shadow/in001210}\caption{Cubicle}
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/shadow/in001850}\caption{Backdoor}	
		\end{subfigureth}
		\caption[Catégorie Shadow]{\textit{Shadow} : Catégorie de vidéos qui présente plus d'ombres que la moyenne.}\label{fig:cdnet:shadow}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/nightVideos/in000060}\caption{WinterStreet}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/nightVideos/in000125}\caption{FluidHighway}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/nightVideos/in000150}\caption{BusyBoulevard}	
		\end{subfigureth}\\
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/nightVideos/in000320}\caption{TramStation}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/nightVideos/in000362}\caption{BridgeEntry}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/nightVideos/in000671}\caption{StreetCornerAtNight}	
		\end{subfigureth}
		\caption[Catégorie Night Videos]{\textit{Night Videos} : Vidéos de nuit avec un contraste fort entre l'obscurité ambiante et les lumières artificielles de l'éclairage public et des phares de voitures.}\label{fig:cdnet:night}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/thermal/in000145}\caption{Park}
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/thermal/in000800}\caption{Corridor}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/thermal/in000840}\caption{DiningRoom}	
		\end{subfigureth}\\
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/thermal/in001050}\caption{Library}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/thermal/in002220}\caption{LakeSide}	
		\end{subfigureth}
		\caption[Catégorie Thermal]{\textit{Thermal} : Ces vidéos ont été prises par une caméra infrarouge et sont en niveau de gris.}\label{fig:cdnet:shadow}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/turbulence/in000425}\caption{Turbulence0}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/turbulence/in000920}\caption{Turbulence1}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/turbulence/in000140}\caption{Turbulence2}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/turbulence/in000195}\caption{Turbulence3}	
		\end{subfigureth}
		\caption[Catégorie Turbulence]{\textit{Turbulence} : Catégorie qui regroupe des vidéos provenant d'une même caméra infrarouge. Les captures ont été faites avec un objectif longue distance filmant des scènes éloignées de 5 à 15 km de l'objectif. Elle présente de nombreuses distorsions et turbulences atmosphériques dûes à la chaleur et à la distance.}\label{fig:cdnet:turbulence}
	\end{figureth}
	
	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/intermittentObjectMotion/in001000}\caption{StreetLight}
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/intermittentObjectMotion/in001300}\caption{Sofa}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/intermittentObjectMotion/in001350}\caption{Tramstop}	
		\end{subfigureth}
		\caption[Catégorie Intermittent Object Motion - Reduced]{\textit{Intermittent Object Motion Reduced} : Cette catégorie comprend des scénarios particuliers dans lesquels le changement est intermittent (c'est à dire qu'un objet passe d'une situation statique à une situation en mouvement et inversement). Dans cette catégorie trois vidéos sur six on été conservées.}\label{fig:cdnet:shadow}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/lowFramerate/in000062}\caption{Turnpike}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/lowFramerate/in000088}\caption{TramCrossroad}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{CDNET/lowFramerate/in000235}\caption{Tunnel Exit}	
		\end{subfigureth}
		\caption[Catégorie Low Framerate - Reduced]{\textit{Low Framerate Reduced} : Cette catégorie regroupe des vidéos avec beaucoup de temps entre les images (entre 1 seconde et 6 secondes entre chaque image). Cela a pour but de pénaliser les approches à partir de flot optique, cependant notre approche n'est pas concernée. Trois vidéos sur les quatre ont été conservées. Seule une vidéo d'une marina a été retirée car la nouveauté (des bateaux) était trop similaire au fond, qui consiste en un grand nombre de bateaux similaires amarrés. Notre approche considérant la nouveauté au sens de l'inattendu. Un objet qui apparaît similaire à des objets déjà présents dans le fond ne rentre pas dans cette interprétation.}\label{fig:cdnet:fps}
	\end{figureth}

	\begin{figureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{intermittentObjectExample/in001285}\caption{Voiture statique}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{intermittentObjectExample/gt001285}\caption{Vérité terrain}
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{intermittentObjectExample/in001815}\caption{Voiture en mouvement}	
		\end{subfigureth}
		\begin{subfigureth}{0.24\textwidth}
			\includegraphics[width=\linewidth]{intermittentObjectExample/gt001815}\caption{Vérité terrain}	
		\end{subfigureth}
		\caption[Différence entre nouveauté and changement]{Ces images sont extraites d'une vidéo de la catégorie \textit{Intermittent Object Motion} et illustrent la différence entre détection de changement et détection de nouveauté. Pour le changement la voiture fait partie du fond pendant une partie de la vidéo car elle est statique. Elle devient objet à détecter à partir du moment où elle commence à se déplacer. Pour la nouveauté, une telle distinction n'est pas possible. Soit elle fait partie du fond, et dans ce cas, même en mouvement elle ne devrait pas être considérée comme nouveauté. Soit elle ne fait pas partie du fond, et dans ce cas elle sera tout le temps considérée comme nouveauté, même lors de la séquence statique.}\label{fig:cdnet:diff}
	\end{figureth}

	\subsection{Métriques utilisées}

	Nous présenterons dans cette section les différentes métriques que l'on a utilisées pour évaluer nos modèles. Elles peuvent être regroupées en deux grandes catégories, la première est proche des modèles évalués (la SOM et les GNG), et essaye de mesurer la qualité d'apprentissage de ceux-ci. La seconde est plus orientée vers la tâche de détection de nouveauté. Cette deuxième catégorie utilise des métriques définies par CDnet pour comparer les résultats avec d'autres modèles. Celles-ci étant en grand nombre, nous nous sommes limités aux trois plus pertinentes qui sont la précision, le rappel et la F-mesure.

	\subsubsection{MQE : Erreur de Quantification Moyenne}

	L'erreur de quantification moyenne (Mean Quantization Error) mesure la qualité de l'apprentissage ou de la reconstruction d'un algorithme de quantification vectorielle. C'est la somme des différences entre tous les vecteurs et leurs représentants, divisée par la dimension et le nombre de vecteurs pour obtenir l'erreur moyenne des composantes.
	
	\begin{equation}
		\text{MQE} = \frac{1}{dn} \sum_{i=0}^{n-1} |v_i - u_i|
	\end{equation}

	Avec $n$ le nombre de vecteurs d'entrée, $d$ la dimension de ces vecteurs, $v_i$ le $i^{\textit{ième}}$ vecteur d'entrée et $u_i$ le prototype du neurone représentant de ce vecteur d'entrée. 

	Il s'agit d'une mesure simple à calculer et à comprendre. Elle présente néanmoins une mauvaise pondération des différences en pénalisant les \textit{outliers} de la même manière que des différences diffuses. Par exemple un pixel qui aura un changement maximal (qui passe de noir à blanc par exemple) aura le même impact sur l'erreur que cent pixels qui passent de 0 à 0,01. Pour des images, le premier changement sera visible pour notre œil, mais pas le second. Nous avons tout de même choisi d'utiliser cette mesure dans nos expériences car elle représente le mieux la différence numérique des vecteurs à leurs représentants avant notre interprétation subjective de ces valeurs en images.

	\subsubsection{PSNR : Peak Signal to Noise Ratio}

	Le PSNR est une mesure très présente dans le domaine de la compression d'images \cite{huynh-psnr, korhonen-psnr}. Elle est similaire à la MQE, à la différence qu'il y a un carré à la place de la valeur absolue. D'où le nom de Mean Squared Quantization Error (MSQE) à calculer pour pouvoir obtenir le PSNR.
	
	\begin{equation}
		\text{MSQE} = \frac{1}{dn} \sum_{i=0}^{n-1} (v_i - u_i)^2
	\end{equation}
	\begin{equation}
		\text{PSNR} = 10 \times \log_{10} \left( \frac{1}{\text{MSQE}} \right)
	\end{equation}

	Avec $n$ le nombre de vecteurs d'entrée, $d$ la dimension de ces vecteurs, $v_i$ le $i^{\textit{ième}}$ vecteur d'entrée et $u_i$ le prototype du neurone représentant de ce vecteur d'entrée.

	Le PSNR est inspiré du domaine du traitement du signal. L'idée est de mesurer le ratio de bruit introduit par la compression (Noise), avec l'intensité maximale du signal (Peak Signal), qui est la valeur maximum d'un pixel (1 dans notre cas). On peut noter que le PSNR fait passer d'un objectif de minimisation à une maximisation. L'ordre des valeurs reste inchangé par rapport au MSQE et le logarithme ajoute un effet de rendement décroissant. Si on a par exemple trois valeurs de MSQE $m_1$, $m_2$ et $m_3$ avec $m_1 < m_2 < m_3$. $m_1$ sera la meilleure (la plus petite) et $m_3$ la moins bonne (la plus grande). Une fois converties en PSNR, les valeurs seront dans l'ordre suivant : $p_1 > p_2 > p_3$, avec $p_1$ étant toujours la meilleure et $p_3$ toujours la moins bonne, mais pour les raisons inverses cette fois : plus un PSNR est grand, mieux c'est. Un autre changement sera que si les intervalles étaient les même entre les trois valeurs, c'est à dire si $m_2 - m_1 = m_3 - m_2$ alors on aura $p_1 - p_2 > p_2 - p_3$, à cause de l'inversion de MSQE.

	Nous avons choisi d'utiliser le PSNR à la place de la MSQE car il est beaucoup plus facilement interprétable ; sa valeur typique étant comprise entre 0 et 100 dB. L'utilisation du carré dans la MSQE entraîne une plus grande pénalisation des \textit{outliers} en comparaison à MQE. Ce n'est malgré tout pas une mesure complète de la qualité d'une image, car elle ne prend pas en compte certains paramètres comme le voisinage par exemple, qui sont importants dans la perception visuelle \ref{sec:sota:vision}. Puisque l'on utilise la distance quadratique dans nos algorithmes de quantification vectorielle, le PSNR est une métrique intéressante car elle intègre ce que notre algorithme d'apprentissage tente de minimiser.
	
	\subsubsection{Précision}

	La précision mesure la proportion de pixels correctement labellisés en tant que nouveauté parmi tous les pixels que le modèle a étiquetés comme nouveauté. La précision est comprise entre 0 et 1 et s'exprime souvent en pourcentage.
	
	\begin{equation}
		\text{Précision} = \frac{\text{Vrais Positifs}}{\text{Vrais Positifs} + \text{Faux Positifs}}
	\end{equation}

	\subsubsection{Rappel}

	Le rappel mesure la proportion de pixels correctement labellisés en tant que nouveauté parmi tous les pixels avec de la nouveauté dans la vérité terrain. Le rappel est compris entre 0 et 1 et s'exprime souvent en pourcentage.

	\begin{equation}
		\text{Rappel} = \frac{\text{Vrais Positifs}}{\text{Vrais Positifs} + \text{Faux Négatifs}}
	\end{equation}

	\subsubsection{F-mesure}

	La précision et le rappel sont deux mesures qui, prises séparément, peuvent être facilement maximisées. Pour avoir une très bonne précision, il suffit de n'inclure que les pixels positifs dont le modèle est sûr dans la carte de saillance, pour réduire la proportion de faux négatifs et ainsi améliorer la précision. Cela entraînera cependant un rappel faible, car le nombre total de vrais positifs diminue. Pour maximiser le rappel, il suffit de faire l'inverse, c'est à dire de catégoriser le plus possible de pixels en positifs dans la carte de saillance et ainsi réduire le nombre de faux négatifs. Cela se fait au détriment de la précision cependant, car le nombre de faux positifs sera en augmentation. La F-mesure \cite{hripcsak-fmeasure} tente d'être une solution à ce problème en combinant la précision et le rappel en un seul nombre à maximiser. Ce n'est cependant pas une mesure sans défauts \cite{powers-fmeasure}.

	\begin{equation}
		\text{F-mesure} = 2 \times \frac{\text{Précision} \times \text{Rappel}}{\text{Précision}+\text{Rappel}}
	\end{equation}

	La formule de la F-mesure est assez simple, mais elle cache un comportement plus complexe. La précision et le rappel étant tous les deux entre 0 et 1, la F-mesure ne peut aussi prendre des valeurs que dans cet intervalle. Car lorsque les deux sont égaux à 1, la formule donne aussi 1. De par les propriétés de la multiplication entre deux nombres entre 0 et 1, la F-mesure favorise les précisions et rappels proches entre elles, et pénalise lorsque les deux valeurs sont éloignées. Ainsi, pour maximiser la F-mesure, augmenter la valeur la plus basse entre la précision et le rappel aura le plus d'effet.
	
	Une propriété notable dans le calcul de la F-mesure est l'absence de distributivité. En pratique, cela implique que la moyenne des F-mesures n'est pas égal à la F-mesure des moyennes de précision et rappel. Cela peut poser problème lorsque l'on essaye d'agréger des valeurs sur plusieurs images par exemple. Il est donc nécessaire de calculer les F-mesures pour chaque image séparément pour ensuite en faire la moyenne. On peut également observer cette propriété dans les résultats présentés dans la section \ref{sec:res:stats}, où la valeur de la F-mesure ne suit pas la formule lorsqu'elle est appliquée aux moyennes des précisions et rappels, car nous avons bien calculé la moyenne des F-mesures.

	\subsubsection{L'œil humain}

	Il n'existe pas de mesure objective pour déterminer la qualité de l'apprentissage d'une quantification vectorielle ou de détection de nouveauté, il n'y a que des estimations. Pour la quantification vectorielle par exemple, le calcul de l'erreur semble naturel, mais il ne capture pas toutes les spécificités des modèles. Notamment lorsque l'on travaille avec des images, le problème déjà évoqué est que certaines erreurs sont visuellement plus perceptibles que d'autres, alors qu'elles ont la même valeur numérique. Mais il y a aussi des propriétés de certains modèles de VQ qui ne peuvent être simplement quantifiées, comme par exemple la gestion des \textit{outliers}. Un algorithme peut par exemple préférer représenter le mieux possible la majorité de la base de données en laissant de côté les \textit{outliers}, ou au contraire faire en sorte que toutes les données soient bien représentées, mais en sacrifiant une meilleure précision sur les données les plus nombreuses. 
	
	Le même problème de subjectivité se présente pour la détection de nouveauté, les mesures quantitatives que nous avons sélectionnées ne représentent pas forcément l'aspect qualitatif de la détection de nouveauté. Par exemple si on a un algorithme qui a pour résultat le contour des objets nouveaux dans une image, la tâche sera bien remplie, mais le score sur les métriques sera mauvais car il sera attendu de l'algorithme qu'il marque de façon positive tous les pixels de l'objet nouveau et non pas seulement le contour. Ce genre de problème peut parfois être résolu par l'utilisation de post processing, le remplissage à partir des contours ici par exemple. Mais ce n'est pas toujours le cas.

	Il est donc nécessaire d'adjoindre à ces métriques objectives, une mesure qualitative du comportement des modèles évalués. Pour les images, nous avons la chance d'être naturellement dotés de très bons capteurs et d'un réseau neuronal biologique performant très entraîné sur des données visuelles. Par conséquent nos interprétations s'appuieront sur le résultat visuel de nos modèles en complément des métriques.

	\subsection{Préparation des données}

	\subsubsection{Image de fond}

	Notre modèle a besoin pour l'apprentissage d'un fond sans nouveauté pour fonctionner. Il nous faut donc une image du fond pour chaque séquence vidéo. Il arrive cependant que des vidéos présentent pendant toute la séquence de la nouveauté, et qu'une image sans perturbation n'existe pas dans la séquence. Pour contrer ce problème, nous avons calculé une image médiane de la séquence pour obtenir une image de fond sans perturbations.

	Pour une mise en pratique potentielle, nous supposons qu'il serait facile d'avoir une image du fond sans perturbations, ou qu'il serait aisé d'appliquer la même méthode en effectuant une capture vidéo et en calculant la médiane lors de l'initialisation.
	
	\subsubsection{Échantillonage de l'évaluation}

	Le calcul des métriques sur les séquences de CDnet se font sur de nombreuses images. Pour la catégorie \textit{baseline} par exemple, il y a en moyenne 1100 images par séquence. Chaque image nécessitant quelques secondes de temps de calculs, utiliser l'intégralité de chaque séquence serait beaucoup trop long pour pouvoir étudier en détail les nombreux paramètres de notre modèle. En partant du principe que des images proches dans le temps sont aussi généralement similaires dans leur contenu, il serait possible d'évaluer une séquence en n'en évaluant qu'un sous-échantillon afin d'obtenir des mesures représentatives du résultat complet. Les images d'une séquence étant indépendantes temporellement les unes des autres dans notre modèle, nous avons choisi de prendre un sous-échantillonnage régulier de la séquence.
	
	Nous avons mené une étude pour déterminer la taille de sous-échantillonnage optimal, que nous présentons dans la figure \ref{fig:params:nbimgs}. Nous avons choisi d'évaluer 105 images par séquences. Cela représente une évaluation entre 7 et 14 fois plus rapide que si on évaluait l'intégralité de la séquence, la vitesse dépendant du nombre d'images total de la séquence complète. La précision des métriques en est impactée, mais la variation ne dépasse pas 0.5\% du résultat.

	\begin{figureth}
		\includegraphics[width=\linewidth]{parameters/number_imgs_evaluated}
		\caption[Échantillonnage de l'évaluation]{Variation de la F-mesure en fonction du nombre d'images analysées. La variation atteint un plateau à partir de 105, qui a été choisi car c'est le plus petit échantillon proche de la moyenne et ne dépassant quasiment pas les 0.5\% de différence. Les sections droites des lignes, surtout vers des grands échantillons, sont à cause d'arrondis sur la taille du pas entier utilisé. Ainsi les échantillons ne sont pas de la taille exacte comme l'indique l'abscisse, mais au moins de la taille indiquée, souvent plus grands. La variabilité qui augmente après 105 n'est pas nécessairement un problème, car l'on ne mesure pas un procédé stochastique, mais la représentativité des images sélectionnées. Ce qui veut dire que la variation due à l'échantillonnage devrait en normalement rester similaire même avec des paramètres différents pour la SOM.}\label{fig:params:nbimgs}
	\end{figureth}

	\subsection{Paramétrages des modèles}

	Notre modèle comporte de nombreux paramètres pour lesquels l'impact sur les performances n'est pas trivial. C'est à dire que changer un paramètre dans un sens pourrait amener à une augmentation des performances à une certaine valeur, et à une diminution des performance à une autre valeur. Il existe également des inter-dépendances entre les paramètres, ce qui signifie que le paramètre $a$ optimal ne sera pas le même pour deux valeurs du paramètre $b$ par exemple. 

	En général, dans ces cas de figure, on effectue une optimisation globale de tous les paramètres en même temps, pour prendre en compte l'interaction entre les paramètres. Cependant, dans notre cas, l'espace de recherche serait très grand (8 dimensions pour 8 paramètres) et nécessiterait un très grand nombre d'exécutions pour le couvrir entièrement. Chacune de nos exécutions durant en moyenne quelques minutes, il n'est pas souhaitable d'en effectuer un trop grand nombre. De plus, il est difficile avec ce genre d'approche de la paramétrisation de comprendre l'impact de chaque paramètre sur le résultat.

	Nous avons ainsi choisi de faire une étude paramétrique en séparant les paramètres le plus possible. Cela nous permet d'analyser chaque paramètre, ou groupe de paramètres en détail pour mieux comprendre leur effet sur le comportement de notre modèle. Cela nous amènera également à pouvoir prédire un comportement dans des espaces paramétriques que nous n'avons pas explorés ; vers quelle valeur les performances convergent-elles si on fait tendre un paramètre vers l'infini par exemple. Cette étude pourra être faite avec un nombre raisonnable d'exécutions avec les moyens matériels que nous avons à disposition. Le défaut sera que l'interaction entre certains paramètres sera difficile à évaluer. Nous optimiserons nos paramètres pour maximiser la \textit{F-mesure}, car c'est la métrique la plus proche de la tâche de détection de nouveauté. La section se conclura par un tableau récapitulatif des paramètres que nous aurons utilisés pour générer nos résultats.

	\subsubsection{Variation aléatoire}

	Les SOM que nous utilisons ne sont pas déterministes, et plusieurs facteurs aléatoires peuvent influencer le résultat d'un apprentissage. Ces deux facteurs sont l'initialisation des poids des neurones, que nous faisons débuter à des valeurs aléatoires entre 0 et 1 pour chaque composante. Mais aussi l'ordre de présentation de la base d'apprentissage qui est aléatoire, et donc varie avec la graine du générateur d'aléatoire paramétrée avant l'apprentissage. Cette dépendance à l'aléatoire implique que les métriques que l'on aura calculées peuvent varier d'un apprentissage à l'autre. Nous avons étudié cette variabilité pour connaître quel serait le plus petit nombre d'exécutions nécessaire pour donner une estimation fiable de la moyenne des résultats pour un ensemble de paramètres. 

	\begin{figureth}
		\includegraphics[width=.6\linewidth]{parameters/randomness_distribution}
		\caption[Effet de l'aléatoire sur les métriques]{Distribution de la F'mesure et du PSNR pour un ensemble de paramètres donnés pour une vidéo. On a découpé l'intervalle de résultats en 9 sections égales. La section numéro 5 a la moyenne en son centre. La largeur de chaque région a été ajustée pour que le maximum soit à la limite haute de la section 9 ou le minimum à la limite basse de la section 1, en choisissant celui qui donnerait les plus grandes sections. L'axe des ordonnées quant à lui donne le nombre d'exécutions incluses dans chaque catégorie, sur 100 exécutions au total. Nous pouvons observer que les distributions suivent une loi normale. Il semblerait que la variabilité de la F-mesure est inférieure à celle de la MSQE.}\label{fig:params:random}
	\end{figureth}

	D'après la figure \ref{fig:params:random}, le comportement aléatoire de notre modèle peut-être assimilé à une loi normale. Cela nous permet, en utilisant la règle empirique, de donner un intervalle de confiance lorsque l'on estimera la moyenne pour nos mesures. Nous supposerons que les variances de la \textit{baseline} sur laquelle nous avons mesuré ces valeurs est du même ordre que sur toutes les vidéos du jeu de données CDNET. Nous supposerons aussi que toutes les distributions suivent une loi normale.

	\begin{tableth}
	\begin{tabular}{|c|c|cccc|}
		\hline
		Video	& Écart-type $\sigma$& $\delta=1\%$ /$2\sigma$ & $\delta=1\%$ /$3\sigma$ & $\delta=0.5\%$ /$2\sigma$ & $\delta=0.5\%$ /$3\sigma$\\
		\hline
		highway & $5.68 \times 10^{-3}$ & $1.3$ & $2.9$ & $5.2$ & $11.6$\\
		office & $9.9 \times 10^{-3}$ & $4.0$ & $8.9$ & $15.8$ & $35.6$\\
		pedestrians & $1.52 \times 10^{-2}$ & $9.2$ & $20.8$ & $36.9$ & $83.1$\\
		PETS2006 & $1.08 \times 10^{-2}$ & $4.7$ & $10.5$ & $18.6$ & $41.9$\\
		\hline
	\end{tabular}
	\caption[Estimations statistiques du nombre de graines requises]{Nombre d'exécutions avec graines aléatoires différentes requises pour que la moyenne des F-mesures de l'échantillon soit au moins d'une distance $\delta$ de la vraie moyenne, avec une probabilité de 95\% pour 2$\sigma$ et 99,7\% pour 3$\sigma$. L'écart type à partir duquel on déduit ces valeurs, a été calculé sur un échantillon de 100 exécutions pour \textit{highway}, et 50 échantillons pour les autres.} \label{tab:nb_seed_stats}
	\end{tableth}

	D'après le tableau \ref{tab:nb_seed_stats}, nous pouvons observer que l'écart type est très variable en fonction de la vidéo que l'on traite. Chaque éxécution pouvant durer plusieurs minutes, il est aussi difficilement concevable de faire nos optimisations en visant un résultat à $\delta = 0.5$ / $3\sigma$, car cela nécessiterait dans certains cas presque 100 exécutions pour chaque ensemble de paramètres. Nous avons ainsi choisi de nous limiter à 8 exécutions avec des graines aléatoires différentes, car les ordinateurs sur lesquels nous expérimentons possèdent 8 cœurs, et que cela nous permet de dépasser le premier seuil de $\delta=1\%$ /$2\sigma$ pour la plupart des vidéos.

	\subsubsection{Optimisation d'$\alpha$ et de $\sigma$}

	Nous avons présenté dans la section \ref{param_som} ces deux paramètres et leurs effets. Ces paramètres sont très dépendants entre eux, car ils pondèrent tous les deux la formule de modification des poids de l'apprentissage. Nous les avons donc optimisés ensemble. La recherche a été faite par un \textit{Tree-structured Parzen Estimator} \cite{bergstra2011algorithms}, sur des vidéos de la \textit{baseline}. Nous avons fait plus de 1000 mesures par vidéo, avec pour chaque mesure, la moyenne de 8 exécutions, deux pour chaque vidéo, pour réduire le bruit statistique. La figure \ref{fig:params:sigopt} explique le choix de $\sigma$ que nous avons fait commencer à 0.7 et finir à 0.015. Puis nous avons relancé une optimisation pour les valeurs d'alpha uniquement pour mieux observer les tendances avec les valeurs fixées de $\sigma$. La figure \ref{fig:params:alphaopt} présente ces résultats. Nous avons choisi pour $\alpha$ un départ à 0.2 et de finir à 0.05.

	Nous pouvons aussi noter l'ordre d'importance des paramètres par leur impact sur le résultat : $\textit{Sigma end} > \textit{Sigma start} > \textit{Alpha end} > \textit{Alpha start}$.

	\begin{figureth}
		\begin{subfigureth}{.75\textwidth}
			\includegraphics[width=\linewidth]{parameters/sigma_end}	
		\end{subfigureth}
		\begin{subfigureth}{.75\textwidth}
			\includegraphics[width=\linewidth]{parameters/sigma_start}	
		\end{subfigureth}
		\caption[Optimisation de $\sigma$]{Le premier paramètre qui a une valeur optimale évidente est \textit{Sigma end}, où une petite valeur proche de zéro semble idéale. Cela s'explique facilement du fait qu'une valeur faible de \textit{Sigma end} amène à ce que lors des dernières époques la SOM va se focaliser sur l'optimisation de la quantification vectorielle au dépend de la topologie, pour obtenir des neurones les plus proches possibles des imagettes qu'ils représentent. Nous l'avons choisi à 0.015.\\
		
		Le second graphique représente un sous échantillon d'expériences sélectionnés avec une valeur de \textit{Sigma end} inférieure à 0.05. En affichant les résultats en fonction de \textit{Sigma start}, on observe une légère préférence pour des valeurs de \textit{Sigma start} assez élevées, c'est à dire dans les alentours de 0.7.}\label{fig:params:sigopt}
	\end{figureth}


	\begin{figureth}
		\begin{subfigureth}{.75\textwidth}
			\includegraphics[width=\linewidth]{parameters/alpha_end}	
		\end{subfigureth}
		\begin{subfigureth}{.75\textwidth}
			\includegraphics[width=\linewidth]{parameters/alpha_start}	
		\end{subfigureth}
		\caption[Optimisation d'$\alpha$]{\textit{Alpha end} semble fonctionner avec une préférence similaire à \textit{Sigma end}. Une faible valeur d'\textit{Alpha end} non nulle semble optimale. En pratique cela se traduit par un ajustement fin des poids des neurones à la fin de l'apprentissage. Nous l'avons mis à 0.05\\
		
		Pour \textit{Alpha start} nous avons à nouveau réduit les expériences à un échantillon dont la valeur d'\textit{Alpha end} est inférieure à 0.2. Il ne semble pas y avoir de préférence forte pour \textit{Alpha start}, nous prendrons donc le minimum à 0.2.}\label{fig:params:alphaopt}
	\end{figureth}


	\subsubsection{Durée de l'apprentissage}

	Le dernier paramètre des SOM qu'il reste à optimiser est le nombre d'époques. Une époque étant un parcours complet de la base d'apprentissage, ce paramètre doit être assez élevé pour que la SOM converge et assez bas pour que l'apprentissage ne soit pas trop long, la durée de celui-ci étant directement proportionnelle au nombre d'époques. Le résultat de nos tests est montré figure \ref{fig:params:nbepochs}. Nous avons choisi 120 époques d'apprentissage pour les SOM.

	\begin{figureth}
		\includegraphics[width=\linewidth]{parameters/number_epochs}
		\caption[Nombre d'époques]{F-mesure en fonction du nombre d'époques sur la \textit{baseline}. Une chose étrange que l'on peut remarquer est qu'une des séquences (PETS2006) a des performances décroissantes avec l'apprentissage. Cela pourrait s'expliquer par un seuil trop haut pour cette séquence, car plus l'apprentissage avance, plus les erreurs sont petites. De plus petites erreurs auront plus de chances d'être en dessous du seuil de décision, et ainsi diminuer le rappel, qui diminue ensuite aussi la F-mesure. Nous avons utilisé la moyenne des séquences croissantes uniquement pour déterminer le moment de convergence de l'apprentissage. Après environ 100 époques, les performances semblent stables et restent dans un intervalle restreint de f-mesure. Nous avons donc choisi 120 époques d'apprentissage pour que la convergence soit atteinte sans trop coûter en calculs.}\label{fig:params:nbepochs}
	\end{figureth}

	\subsubsection{Récapitulatif}\label{sec:param:recap}

	\begin{tableth}
	\caption[Récapitulatif des paramètres SOM]{Récapitulatif des paramètres SOM}
	\begin{tabular}{|cc|cc|c|c|c|}
		\hline
		$\alpha$-start	& $\alpha$-end & $\sigma$-start & $\sigma$-end & Époques & Images par séquence\\
		\hline
		0.2 & 0.05 & 0.7 & 0.015 & 120 & 105\\
		\hline
	\end{tabular}\label{tab:recap:param}
	\end{tableth}
	
	\subsubsection{Paramètres des GNG}

	Nous avons choisi d'utiliser des GNG pour montrer que notre méthode est généralisable à d'autres modèles avec quantification vectorielle et topologie. Ainsi, nous nous sommes contentés pour la paramétrisation d'utiliser de nouveau un \textit{Tree-structured Parzen Estimator} pour optimiser l'intégralité des paramètres en une fois. Leurs valeurs sont présentées dans le tableau \ref{tab:gng:param}.

	\begin{tableth}
	\caption[Paramètres GNG]{Paramètres des GNG}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$\epsilon_{bmu}$ & $\epsilon_{j}$ & $a_{max}$ & $\alpha$ & $d$ (erreur globale) & Époques\\
		\hline
		0.4 & 0.01 & 85 & 0.6 & 0.3 & 110\\
		\hline
	\end{tabular}\label{tab:gng:param}
	\end{tableth}



	\newpage

	\section{Résultats expérimentaux}

	Nous présentons dans cette section tous les résultats quantitatifs et visuels que nous avons obtenus avec notre méthode sur CDnet. Cette section est découpée en quatre parties. La première présente l'impact du seuil, du nombre de neurones et de la taille des imagettes sur les performances et les différences entre les vidéos lorsque l'on fait varier ces paramètres. La seconde présente les résultats complets sur CDNET et les comparera avec l'état de l'art. La troisième présente visuellement les images que produit notre modèle. La dernière est consacrée aux discussions et perspectives.

	Le seuil, la taille des imagettes et le nombre de neurones auraient pu faire partie de la section des paramètres. Cependant compte tenu de l'importance qu'ils ont sur le résultat et des informations et perspectives qu'ils nous donnent, nous les avons inclus dans la partie résultats.

	Cette section est basée sur un ensemble d'exécutions sur l'intégralité de CDnet. Les SOM sont exécutées les paramètres définis dans la section précédente \ref{sec:param:recap}. Au total, cette expérience contient 2520 exécutions pour la SOM, et le même nombre pour les GNG. 

	\subsection{Seuil de décision}

	\begin{figureth}
		\includegraphics[width=.48\linewidth]{parameters/seuil_heatmap}
		\includegraphics[width=.48\linewidth]{parameters/seuil_fmeasure_heatmap}
		\caption[Seuil de décision]{La figure de gauche indique le seuil optimal pour chaque couple de taille de SOM et de taille d'imagettes que l'on a testé. On peut remarquer que le seuil maximisant la F-mesure diminue plus la taille de la carte est grande, car il y a moins d'erreurs, et qu'il augmente avec la taille des imagettes, car cela augmente les pertes dues à la compression.\\

		La figure de droite montre la F-mesure complète (sur l'ensemble de CDnet) pour ce seuil optimal. Le maximum est obtenu pour $25\times25$ neurones, $20\times20$ pixels et un seuil de $7$.}\label{fig:res:seuil}

	\end{figureth}
		\begin{figureth}
		\begin{subfigureth}{.48\textwidth}
			\includegraphics[width=\linewidth]{parameters/seuil_pedestrians}\caption{Pedestrians}
		\end{subfigureth}
		\begin{subfigureth}{.48\textwidth}
			\includegraphics[width=\linewidth]{parameters/seuil_pets2006}\caption{PETS2006}
		\end{subfigureth}
		\caption[Seuil de décision en fonction de la séquence]{Les deux figures présentent les seuils optimaux pour deux séquences différentes, qui montrent la forte variabilité de ce paramètre.}\label{fig:res:seuil_seq}
	\end{figureth}

	Le seuil de décision est ce qui permet de passer de notre carte de saillance qui contient des valeurs entre 0 et 255 à la carte de détection binaire. Toutes les valeurs en dessous ou égales au seuil seront considérées comme fond, et les valeurs au dessus du seuil comme nouveauté. La figure \ref{fig:res:seuil} présente nos résultats pour le seuil global, et la figure \ref{fig:res:seuil_seq} montre que l'optimal est très variable en fonction de la séquence choisie.

	\subsection{Nombre de neurones et taille des imagettes}

	\subsubsection{Baseline}

	\begin{figureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/som/baseline_highway}\caption{Highway}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/som/baseline_office}\caption{Office}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/som/baseline_pedestrians}\caption{Pedestrians}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/som/baseline_PETS2006}\caption{PETS2006}
		\end{subfigureth}
		\caption[Nombre de neurones et de taille des imagettes, SOM/baseline]{F-mesure en fonction du nombre de neurones et de la taille des imagettes pour les séquences de la \textit{baseline} avec une SOM.}\label{fig:res:F-mesure3D-baselineSOM}
	\end{figureth}

	La figure \ref{fig:res:F-mesure3D-baselineSOM} montre une différence significative de comportement entre les séquences vidéo. La taille des imagettes semble être le facteur de variations le plus important. Pour \textit{Pedestrians} par exemple, une taille d'imagette petite de $10\times10$ donnera les meilleures performances, alors que pour \textit{PETS2006}, ce sont les grosses imagettes de $25\times25$ qui sont optimales. La variabilité est aussi très différente en fonction de la séquence considérée. \textit{Highway} et \textit{Office} sont relativement stables et peu sensibles aux changements de taille d'imagettes et de neurones, tant que ceux-ci ne sont pas trop petits. Au contraire de \textit{PETS2006} pour qui les performances peuvent aller du simple au double en fonction de la taille choisie.

	Pour le nombre de neurones, l'optimal semble toujours être du côté où les cartes sont les plus grandes. Cependant les gains apportés par une carte plus grande sont très variables. Généralement importants au début pour une carte petite, les valeurs atteignent rapidement un plateau et n'améliorent que peu la détection de nouveauté. C'est une information importante pour une détection rapide et efficace, car le nombre de neurones dans la carte est un facteur important du coût en calcul de la SOM.

	\begin{figureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/gng/baseline_highway}\caption{Highway}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/gng/baseline_office}\caption{Office}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/gng/baseline_pedestrians}\caption{Pedestrians}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/gng/baseline_PETS2006}\caption{PETS2006}
		\end{subfigureth}
		\caption[Nombre de neurones et de taille des imagettes, GNG/baseline]{F-mesure en fonction du nombre de neurones et de la taille des imagettes pour les séquences de la \textit{baseline} avec un GNG. Les GNG n'ayant pas une topologie carrée comme les SOM, il suffit de mettre au carré la taille de la carte pour obtenir le nombre de neurones utilisés par le GNG.}\label{fig:res:F-mesure3D-baselineGNG}
	\end{figureth}

	Les résultats pour les GNG de la figure \ref{fig:res:F-mesure3D-baselineGNG} montrent un comportement plus chaotique. Les variations sont particulièrement grandes entre plusieurs exécutions, mais on peut tout de même discerner les mêmes optimums de tailles d'imagettes que pour les SOM. \textit{Pedestrians} préfère encore des imagettes petites (10 par 10), et \textit{PETS2006} des imagettes grandes (25 par 25). Cela semble indiquer que ces optimums ne sont pas dépendantes des modèles (SOM, GNG) mais bien seulement de la vidéo ou de la nouveauté présente dans la vidéo.

	\begin{figureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/psnr/baseline_highway}\caption{Highway}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/psnr/baseline_office}\caption{Office}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/psnr/baseline_pedestrians}\caption{Pedestrians}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/psnr/baseline_PETS2006}\caption{PETS2006}
		\end{subfigureth}
		\caption[Nombre de neurones et de taille des imagettes, PSNR/SOM/baseline]{PSNR en fonction du nombre de neurones et de la taille des imagettes pour les séquences de la \textit{baseline} avec une SOM.}\label{fig:res:psnr-baselineSOM}
	\end{figureth}

	La variation du PSNR est montrée dans la figure \ref{fig:res:psnr-baselineSOM}. On peut y observer que les optimums pour la compression sont les mêmes pour toutes les vidéos, et donc différentes d'optimums pour les F-mesures. La compression donne le meilleur résultat pour des petites imagettes de $5\times5$ et s'améliore aussi légèrement avec le nombre de neurones. La conclusion est que la qualité de compression et la détection de nouveauté ne sont pas corrélées. Améliorer le résultat de la compression n'amènera ainsi pas forcément des gains au niveau de la détection de nouveauté.

	Ce résultat peut s'expliquer par le fait que les petites imagettes reconstruisent bien le fond, mais aussi la nouveauté. Une imagette de 20 par 20 pixels avec de la nouveauté aura par exemple une distance plus grande avec un seul neurone et son vecteur prototype de 20 par 20 pixels que de quatre neurones de 5 par 5 pixels, chacun d'entre eux étant la BMU de leur portion de l'imagette. Ainsi, la saillance de la nouveauté est réduite en même temps que l'erreur de reconstruction avec des imagettes plus petites, ce qui peut amener à une F-mesure réduite.
	
	\subsubsection{Global}

	\begin{figureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/global_som}\caption{SOM}
		\end{subfigureth}
		\begin{subfigureth}{.45\textwidth}
			\includegraphics[width=\linewidth]{resultats/neurones_imagettes/global_gng}\caption{GNG}
		\end{subfigureth}
		\caption[Nombre de neurones et de taille des imagettes, Global]{Moyenne des F-mesure en fonction du nombre de neurones et de la taille des imagettes pour l'ensemble des vidéos de notre jeu de données.}\label{fig:res:F-mesure3D-global}
	\end{figureth}

	Lorsque sont agrégés toutes les séquences vidéos, les graphes de la figure \ref{fig:res:F-mesure3D-global} montrent les tendances globales sur notre modèle. La détection de nouveauté avec la SOM fonctionne le mieux avec des tailles d'imagettes proches de $20\times20$. Augmenter le nombre de neurones est quant à lui toujours bénéfique, mais amène des gains décroissants. Les résultats pour les GNG sont plus étranges, car ils sont à leur maximum pour des valeurs intermédiaires de neurones et d'imagettes, plus petites que celles de la SOM. Il s'agit peut être uniquement d'un résultat de la forte variabilité observée dans les vidéos individuelles qui se répercute dans la moyenne globale.

	\subsection{Résultats complets}\label{sec:res:stats}

	\begin{tableth}
    \begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		& \multicolumn{3}{c|}{SOM} & SOM Max & GNG & GNG Max\\
        \hline
        \textbf{Categories}/vidéos & Précision & Rappel & F-mesure & F-mesure & F-mesure & F-mesure\\
        \hline
        \textbf{Baseline} & 59.2\% & 72.9\% & 63.9\% & 73.4\% & \textbf{67.0\%} & \textbf{76.4\%} \\
		\hline
		Highway & 63.9\% & 75.7\% & 69.3\% & 78.9\% & 75.9\% & 83.2\% \\
		Office & 71.2\% & 71.0\% & 71.1\% & 74.4\% & 75.9\% & 77.8\% \\
		Pedestrians & 41.4\% & 87.0\% & 56.1\% & 76.7\% & 60.0\% & 78.4\% \\
		PETS2006 & 60.1\% & 57.9\% & 59.0\% & 63.7\% & 56.2\% & 66.4\% \\
		\hline
        \textbf{Bad weather} & 32.6\% & 57.8\% & \textbf{38.9\%} & 48.0\% & 34.8\% & \textbf{54.5\%} \\
		\hline
		Blizzard & 19.4\% & 48.9\% & 27.8\% & 37.3\% & 14.4\% & 47.8\% \\
		Skating & 72.5\% & 68.3\% & 70.3\% & 72.7\% & 74.3\% & 76.1\% \\
		SnowFall & 17.1\% & 57.6\% & 26.4\% & 37.9\% & 24.9\% & 45.9\% \\
		WetSnow & 21.5\% & 56.4\% & 31.1\% & 44.2\% & 25.7\% & 48.1\% \\
		\hline
        \textbf{Camera jitters} & 37.6\% & 65.2\% & \textbf{46.5\%} & 51.7\% & 36.6\% & \textbf{52.3\%} \\
		\hline
		Badminton & 49.2\% & 81.3\% & 61.3\% & 68.6\% & 62.1\% & 68.9\% \\
		Boulevard & 47.9\% & 53.1\% & 50.4\% & 51.8\% & 34.7\% & 51.6\% \\
		Sidewalk & 14.7\% & 45.4\% & 22.2\% & 27.4\% & 14.5\% & 30.9\% \\
		Traffic & 38.5\% & 80.9\% & 52.1\% & 59.0\% & 35.1\% & 57.9\% \\
		\hline
        \textbf{Dynamic bkg} & 8.3\% & 67.1\% & \textbf{14.1\%} & 30.1\% & 12.4\% & \textbf{40.0\%} \\
		\hline
		Boats & 2.7\% & 79.8\% & 5.2\% & 34.2\% & 4.2\% & 62.8\% \\
		Canoe & 16.7\% & 92.4\% & 28.2\% & 56.3\% & 26.0\% & 74.2\% \\
		Fall & 8.8\% & 54.1\% & 15.2\% & 21.2\% & 8.5\% & 18.0\% \\
		Fountain01 & 1.1\% & 43.5\% & 2.1\% & 3.1\% & 1.9\% & 9.8\% \\
		Fountain02 & 3.1\% & 55.3\% & 5.9\% & 21.2\% & 5.0\% & 19.5\% \\
		Overpass & 17.2\% & 77.8\% & 28.2\% & 44.6\% & 28.7\% & 55.7\% \\
		\hline
        \textbf{Night videos} & 27.8\% & 47.1\% & \textbf{32.1\%} & \textbf{38.8\%} & 32.0\% & 37.1\% \\
		\hline
		BridgeEntry & 10.8\% & 33.9\% & 16.4\% & 26.3\% & 12.9\% & 21.5\% \\
		BusyBoulvard & 40.5\% & 28.9\% & 33.8\% & 39.5\% & 31.5\% & 33.1\% \\
		FluidHighway & 19.1\% & 55.7\% & 28.5\% & 31.2\% & 29.6\% & 30.6\% \\
		StreetCornerAtNight & 21.8\% & 59.7\% & 31.9\% & 39.7\% & 29.6\% & 35.7\% \\
		TramStation & 47.9\% & 41.1\% & 44.2\% & 56.4\% & 49.5\% & 62.1\% \\
		WinterStreet & 26.7\% & 63.4\% & 37.6\% & 39.8\% & 38.6\% & 39.7\% \\
		\hline
        \textbf{Shadow} & 48.6\% & 64.2\% & \textbf{52.8\%} & 59.8\% & 49.7\% & \textbf{62.0\%} \\
		\hline
		Backdoor & 27.1\% & 76.2\% & 40.0\% & 49.4\% & 36.6\% & 48.7\% \\
		Bungalows & 54.9\% & 49.8\% & 52.2\% & 72.1\% & 63.0\% & 72.2\% \\
		BusStation & 64.6\% & 67.3\% & 66.0\% & 66.1\% & 54.4\% & 66.3\% \\
		CopyMachine & 77.7\% & 67.1\% & 72.0\% & 79.2\% & 58.9\% & 78.0\% \\
		Cubicle & 17.0\% & 42.9\% & 24.3\% & 26.9\% & 19.5\% & 39.5\% \\
		PeopleInShade & 50.0\% & 81.7\% & 62.1\% & 65.0\% & 65.7\% & 67.3\% \\
		\hline
        \textbf{Thermal} & 52.0\% & 66.2\% & \textbf{54.0\%} & 65.5\% & 51.4\% & \textbf{66.5\%} \\
		\hline
		Corridor & 32.4\% & 87.3\% & 47.2\% & 75.4\% & 26.4\% & 72.8\% \\
		DiningRoom & 67.4\% & 55.7\% & 61.0\% & 70.8\% & 65.6\% & 72.5\% \\
		LakeSide & 13.8\% & 53.0\% & 21.9\% & 33.4\% & 14.7\% & 30.4\% \\
		Library & 85.9\% & 88.4\% & 87.1\% & 91.5\% & 87.8\% & 94.2\% \\
		Park & 60.7\% & 46.4\% & 52.6\% & 56.4\% & 62.6\% & 62.8\% \\
		\hline
        \textbf{Global} & 41.3\% & 62.3\% & \textbf{43.5\%} & 54.6\% & 41.9\% & \textbf{57.0\%} \\
    	\hline
	\end{tabular}
	\caption{Résultats complets sur CDNET de notre détection de nouveauté}
	\label{tab:res:complet1}
	\end{tableth}


	\begin{tableth}
    \begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		& \multicolumn{3}{c|}{SOM Global} & SOM Max & GNG Global & GNG Max\\
        \hline
        \textbf{Categories}/vidéos & Précision & Rappel & F-mesure & F-mesure & F-mesure & F-mesure\\
		\hline
        \textbf{Turbulence} & 8.7\% & 69.6\% & 12.4\% & 35.1\% & \textbf{14.9\%} & \textbf{36.9\%} \\
		\hline
		Turbulence0 & 1.5\% & 82.7\% & 3.0\% & 28.0\% & 2.9\% & 24.3\% \\
		Turbulence1 & 6.0\% & 68.2\% & 11.0\% & 34.1\% & 13.5\% & 34.6\% \\
		Turbulence2 & 1.3\% & 81.8\% & 2.6\% & 33.1\% & 0.9\% & 35.4\% \\
		Turbulence3 & 25.9\% & 45.6\% & 33.1\% & 45.1\% & 42.3\% & 53.1\% \\
		\hline
		\textbf{Inter. motion [r]} & 68.0\% & 46.7\% & 52.0\% & \textbf{69.0\%} & \textbf{57.7\%} & 68.5\% \\
		\hline
		Sofa & 61.1\% & 49.0\% & 54.4\% & 67.7\% & 58.4\% & 72.2\% \\
		StreetLight & 64.6\% & 68.4\% & 66.4\% & 85.1\% & 77.4\% & 83.3\% \\
		Tramstop & 78.4\% & 22.7\% & 35.3\% & 54.3\% & 37.1\% & 49.9\% \\
		\hline
        \textbf{Low framerate [r]} & 70.5\% & 66.4\% & \textbf{68.0\%} & 74.3\% & 62.7\% & \textbf{75.4\%} \\
		\hline
		TramCrossroad & 61.4\% & 72.9\% & 66.6\% & 68.7\% & 67.5\% & 69.3\% \\
		TunnelExit & 77.3\% & 63.7\% & 69.9\% & 75.4\% & 48.1\% & 74.5\% \\
		Turnpike & 73.0\% & 62.7\% & 67.4\% & 78.7\% & 72.5\% & 82.4\% \\
        \hline
        \textbf{Global} & 41.3\% & 62.3\% & \textbf{43.5\%} & 54.6\% & 41.9\% & \textbf{57.0\%} \\
    	\hline
	\end{tabular}
	\caption{Résultats complets sur CDNET de notre détection de nouveauté - Suite}
	\label{tab:res:complet2}
	\end{tableth}

	Les résultats complets sont présentés dans les tableaux \ref{tab:res:complet1} et \ref{tab:res:complet2}. Les versions \textit{Global} indiquent que l'on a choisi les valeurs de seuil, de taille de carte et d'imagettes qui étaient optimales sur l'ensemble de CDNET, c'est à dire $25\times25$ pour les tailles de carte et $20\times20$ d'imagettes et un seuil à $7$ pour la SOM. Des imagettes de $15\times15$ pixels, $20\times20$ neurones et un seuil de $5$ pour les GNG. Les versions \textit{Max} sont celles où on a pris la valeur optimale pour chaque vidéo.

	La première chose que l'on peut remarquer, c'est que pour 8 catégories sur 10, le rappel est toujours meilleur que la précision. Pour seulement 13 vidéos sur 45, la précision est meilleure. Notre modèle se révèle globalement plus sensible que précis, même si les résultats dépendent des séquences évaluées. Comme par exemple avec \textit{Tramstop} qui a une précision trois fois supérieure à son rappel.

	On peut également remarquer que notre modèle a beaucoup de mal dans les catégories avec un fond dynamique, c'est à dire avec des feuillages bruissants au vent ou des reflets dans l'eau, comme \textit{Dynamic background} et \textit{Turbulence} avec des précisions ne dépassant pas les 10\%. Mais des légers mouvements de caméras ne pénalisent que peu la F-mesure de notre modèle dans \textit{Camera jitters}. Les autres catégories sont aux alentours de la moyenne globale de la SOM.

	La version Max de la SOM présente des pistes intéressantes. Elle gagne 10\% de performance supplémentaire au global. Cela montre que des gains importants peuvent être faits en ajustant automatiquement le nombre de neurones, la taille des imagettes et le seuil par vidéo. Les gains les plus importants se trouvent dans les catégories dynamiques, avec \textit{Dynamic background} où la F-mesure a été doublée et \textit{Turbulence} où elle a été triplée. On suppose que le seuil est le facteur le plus influent dans ces cas là, car il permet de supprimer une grande partie du bruit environnemental et ainsi d'améliorer la précision pour ces vidéos là.

	Les GNG ont des performances similaires à la SOM, et réussissent et échouent dans les mêmes catégories. Au global, ils sont légèrement moins bons que les SOM avec 41,9\% de F-mesure contre 43,5\%. La version Max cependant arrive à 57\%, soit au dessus de la version SOM Max à 54,6\%. Cela provient peut-être de la plus forte variabilité observée dans les résultats des GNG, que l'on a par conséquent favorisée en choisissant le maximum pour chaque vidéo.

	\subsubsection{Comparaison avec l'état de l'art}

	CDNET étant un jeu de données très utilisé, il existe de nombreux algorithmes de détection de changement qui ont été testés sur celui-ci. Leur site web\footnote{http://changedetection.net/} référence les résultats obtenus dans la littérature. Ceux-ci sont généralement obtenus en utilisant du post processing pour maximiser les performances. Vu que nous n'en utilisons pas, nous avons choisi de se comparer à \cite{xu2016background}, qui est une review qui reproduit les résultats de méthodes communes de détection de changement, sans pré ni post processing. Nous avons reproduit leurs résultats dans le tableau \ref{tab:res:compare}.

	\begin{tableth}
    \begin{tabular}{|c|ccc|}
		\hline
		Modèle & Précision & Rappel & F-mesure\\
		\hline
		SACON & 46.3\% & 38.2\% & 24.0\%\\
		KDE & 37.6\% & \textbf{68.7}\% & 40.8\%\\
		CodeBook & 61.2\% & 38.6\% & 41.1\% \\
		Vibe & 65.3\% & 46.5\% & 47.2\% \\
		GMM & 49.9\% & 62.3\% & 47.6\% \\
		AGMM & 63.4\% & 56.0\% & 53.9\% \\
		PBAS & \textbf{70.9}\% & 50.8\% & 55.1\% \\
		SOBS & 66.7\% & 60.1\% & \textbf{56.4}\% \\
		\hline
		GNG Global & 39.6\% & 66.1\% & 41.9\% \\
		SOM Global & 41.3\% & 62.3\% & 43.5\% \\
		\hline
	\end{tabular}
	\caption{Comparatif avec d'autres modèles de détection de changement sur CDNET}
	\label{tab:res:compare}
	\end{tableth}

	Notre méthode est avant dernière en précision, 2\textsuperscript{ème} ex-æquo en rappel et 6\textsuperscript{ème} sur 9 en F-mesure. Les chiffres indiqués ne sont partiellement comparables, car nos valeurs ont été calculées sur une partie de CDNET, mais l'ordre de grandeur devrait environ être le même. En comparaison des autres modèles, la précision est le plus gros facteur limitant de notre approche. Il faut aussi remarquer que les modèles utilisés dans cette review sont tous "classiques", et le meilleur algorithme actuel BSUV-Net 2 de \cite{tezcan2021bsuv}, est un réseau de neurones qui obtient 83.9\% en F-mesure sur CDNET, en utilisant de l'augmentation de données.

	\subsection{Visualisation des résultats}
	
	Dans cette section nous avons regroupé les résultats de détection de notre modèle. La figure \ref{fig:res:visu-baseline} montre les résultats qui ont obtenu un bon score de détection. On peut y voir que le découpage en bloc de la SOM est présent dans le résultat de la détection. Cela peut expliquer en partie le rappel élevé et la précision plus faible car notre modèle aura tendance à déborder au delà de la stricte nouveauté. On remarquera aussi qu'il n'y a pas de bruit dans nos détections. Le découpage entre zone de nouveauté et zone de fond est très net, même si il présente quelques erreurs, comme une zone de nouveauté qui n'apparaît pas, ou une zone de fond qui apparaît comme nouveauté.

	\begin{figureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/baseline/highway/in001647}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/baseline/highway/bin001647}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/baseline/highway/gt001647}
		\end{subfigureth}

		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/baseline/backdoor/in001840} \caption{Entrée}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/baseline/backdoor/bin001840} \caption{SOM-Global}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/baseline/backdoor/gt001840} \caption{Vérité terrain}
		\end{subfigureth}

		\caption[Visualisation résultats normaux]{Visualisations de résultats normaux de notre modèle. On peut y voir le découpage en bloc dû à l'utilisation d'imagettes et l'absence de bruit autour des nouveautés à détecter, mais avec quelques erreurs tout de même.}\label{fig:res:visu-baseline}
	\end{figureth}

	La figure \ref{fig:res:visu-dynamic} nous présente des cas de fond dynamiques où notre modèle a eu particulièrement du mal, notamment avec une précision très faible. On peut observer que le mouvement présent dans le fond est confondu avec de la nouveauté. Le mouvement étant permanent, cela cause un nombre très important de faux positifs, ce qui réduit significativement la précision.  

	\begin{figureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/canoe/in000911}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/canoe/bin000911}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/canoe/gt000911}
		\end{subfigureth}

		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/fountain1/in001149}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/fountain1/bin001149} 
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/fountain1/gt001149} 
		\end{subfigureth}

		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/fall/in001504} \caption{Entrée}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/fall/bin001504} \caption{SOM-Global}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/dynamic/fall/gt001504} \caption{Vérité terrain}
		\end{subfigureth}
		\caption[Visualisation résultats dynamiques]{Résultats sur des séquences avec un fond dynamique. Les changements trop importants du fond sont constamment considérés en nouveauté et ainsi réduisent drastiquement la précision.}\label{fig:res:visu-dynamic}
	\end{figureth}

	La figure \ref{fig:res:visu-normalisation} présente un des défauts de la normalisation de la carte de saillance, ce qui se traduit par un fort taux de bruit en l'absence de nouveauté, comme expliqué dans la section \ref{seq:combinaison}.

	\begin{figureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/normalisation/corridor/in002386}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/normalisation/corridor/bin002386}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/normalisation/corridor/gt002386}
		\end{subfigureth}

		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/normalisation/corridor/in002202} \caption{Entrée}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/normalisation/corridor/bin002202} \caption{SOM-Global}
		\end{subfigureth}
		\begin{subfigureth}{.32\textwidth}
			\includegraphics[width=\linewidth]{resultats/visu/normalisation/corridor/gt002202} \caption{Vérité terrain}
		\end{subfigureth}

		\caption[Visualisation des effets de la normalisation]{Exemple d'un cas où le fait de normaliser la carte de saillance pose problème. Dans la première ligne, l'objet est correctement détecté avec les reflets absents de la vérité terrain, et avec très peu de bruit. La seconde ligne sans objet à détecter fait fortement ressortir le bruit à la place.}\label{fig:res:visu-normalisation}
	\end{figureth}

	\subsection{Interprétations}

	Les éléments que nous avons présentés montrent que la détection de nouveauté en utilisant la quantification vectorielle et la topologie d'une SOM fonctionne. Quantitativement parlant la précision de notre détection de nouveauté est loin derrière l'état de l'art, que ce soit des modèles classiques programmés ou des réseaux de neurones apprenants. Il faut cependant remarquer que la comparaison pixel à pixel entre la vérité terrain et l'image de détection est particulièrement désavantageuse pour notre modèle, du fait d'une recherche de nouveauté au niveau des imagettes plutôt que du pixel. Si l'on ne compare que de façon binaire si un objet à été détecté ou non, nos résultats pourraient être plus proches des résultats autres méthodes.
	
	Notre modèle n'étant particulièrement pas adapté pour les fonds dynamiques, l'hypothèse que nous avons faite dans la section \ref{sec:images} ne s'est donc pas concrétisée. Cette hypothèse supposait que la SOM serait capable de généraliser un environnement présent dans l'image de fond, et que tout mouvement dans cet environnement ne serait pas saillant du fait qu'il ait été appris et généralisé par la SOM. En pratique, un mouvement dans une zone de fond entraîne des changements très importants sur tous les pixels d'une imagette, ce qui se traduit par une grande différence au niveau du calcul de la distance avec les neurones, même si visuellement les deux imagettes sont similaires. Par conséquent, les différences seront élevées lorsqu'un fond dynamique est présent, contrairement à l'hypothèse formulée initialement.

	La piste que nous considérons la plus prometteuse pour résoudre ce problème serait de modifier le calcul de distances dans la SOM pour mieux refléter les différences dans les images. Une mesure comme la Structual Similarity (SSIM) \cite{wang2004image}, qui est un calcul de différences mieux adapté pour les images, même si elle ne suit pas les propriétés d'une distance. Une autre option plus neuronale serait d'essayer d'apprendre les types des différences qui peuvent se produire au cours du temps, comme les remous de l'eau par exemple, et ainsi ajouter une notion temporelle à notre modèle, qui n'en possède pas encore.

	Nous avons aussi montré avec les GNG que notre approche est facilement généralisable à d'autres modèles de quantification vectorielle avec topologie. Des variantes de SOM ou de GNG pourraient ainsi être essayées pour cette tâche pour potentiellement amener à de meilleurs résultats que la SOM classique sur cette application.

	%\newpage
	% \section{Conclusion}

	% Nous avons vu dans ce chapitre une méthode pour apprendre l'environnement d'une image avec une SOM et comment utiliser cette connaissance pour effectuer une tâche de détection de nouveauté.

	% Les performances sont en dessous de l'état de l'art, mais notre approche à pu montrer une certaine agilité des cartes auto-organisatrices à s'adapter à la tâche qu'on lui a fait faire, sans avoir à lui adjoindre d'autres mécanismes de détections de nouveauté complémentaires. Notre modèle n'a fait que manipuler les informations déjà présentes dans la carte pour effectuer la détection de nouveauté. Nous n'avons pas ajouté de pré-processing, ni de post-processing pour améliorer le résultat, il vient uniquement de la SOM.

	% Il faut aussi remarquer que l'information est présente à de multiples niveaux dans les SOM. Non seulement dans la quantification vectorielle, mais aussi dans la topologie (et peut-être d'autres encore). L'idée que la topologie, c'est à dire l'organisation des neurones contienne de l'information nous rapproche des théories de l'émergence que nous avons présenté dans le contexte.

	%\newpage		
\bibliographystyle{francaissc}
\bibliography{Chapitre2/Biblio}